REWRITE_PROMPT_TEMPLATES = {
    "gate_rewrite_v1": (
        "You are rewriting a search query for reasoning-intensive retrieval.\n\n"
        "Key idea:\n"
        "- Assume you already have the correct answer to the user query.\n"
        "- The documents we want are the ones that would be used as core evidence or justification for that answer.\n"
        "- These evidence documents are often more abstract than the surface query.\n\n"
        "Task:\n"
        "- First, write a 1-2 sentence Plan that states the user's core intent and what kind of evidence would justify the answer.\n"
        "- Use the context summaries only as hints (not ground truth).\n"
        "- Produce 2-5 distinct Possible_Answer_Docs that could serve as evidence for the assumed correct answer.\n"
        "- Avoid near-duplicates; keep each item short and retrieval-friendly.\n"
        "- Evidence forms may include (not exhaustive): theory/mechanism, entity/fact, analogy/example, method/metric, canonical reference.\n\n"
        "Output JSON only:\n"
        "{\n"
        "  \"Plan\": \"short reasoning\",\n"
        "  \"Possible_Answer_Docs\": [\n"
        "    \"...\",\n"
        "    \"...\",\n"
        "    \"...\"\n"
        "  ]\n"
        "}\n\n"
        "Original Query:\n{original_query}\n\n"
        "Previous Rewritten Query:\n{previous_rewrite}\n\n"
        "Context Summaries:\n{gate_descs}\n"
    ),
    "gate_rewrite_schema_v1": (
        "You are rewriting a search query for reasoning-intensive retrieval.\n\n"
        "Key idea:\n"
        "- Assume you already have the correct answer to the user query.\n"
        "- The documents we want are the ones that would be used as core evidence or justification for that answer.\n"
        "- These evidence documents are often more abstract than the surface query.\n\n"
        "Task:\n"
        "- First, write a 1-2 sentence Plan that states the user's core intent, answer, and what kind of evidence would justify the answer.\n"
        "- Produce 2-5 distinct Possible_Answer_Docs, guided by the given categories.\n"
        "Output JSON only:\n"
        "{\n"
        "  \"Plan\": \"short reasoning\",\n"
        "  \"Possible_Answer_Docs\": {\n"
        "{schema_kv_template}\n"
        "  }\n"
        "}\n\n"
        "Original Query:\n{original_query}\n\n"
        "Previous Rewritten Query:\n{previous_rewrite}\n\n"
        "Context Summaries:\n{gate_descs}\n"
    ),
    "round3_action_v1": (
        "You are rewriting a search query for reasoning-intensive retrieval.\n\n"
        "Goal:\n"
        "- Decide whether to stay local (EXPLOIT) or change direction (EXPLORE).\n"
        "- If evidence supports the current hypothesis (previous rewrite), choose EXPLOIT.\n"
        "- If evidence is missing, contradictory, or points elsewhere, choose EXPLORE.\n\n"
        "Task:\n"
        "- Write a short Plan that follows these steps:\n"
        "  1. Identify the user's intent and answer type.\n"
        "  2. Abstraction: infer which academic terms, theories, models, or canonical methods would be cited in a correct answer.\n"
        "  3. Verification: compare the evidence against your current hypothesis.\n"
        "     - If supported -> EXPLOIT.\n"
        "     - If missing/contradictory -> EXPLORE and treat the previous rewrite as a negative constraint.\n"
        "  4. Ensure every generated document is abstract evidence (NOT surface-level restatements).\n"
        "- Action-specific behavior:\n"
        "  - EXPLOIT: Refine the current hypothesis and **use key terms from evidence as anchors**, while staying abstract.\n"
        "  - EXPLORE: Pivot to a new abstract hypothesis (different theory/framework/entity name), avoiding the failed direction.\n"
        "- Produce 3-5 distinct Possible_Answer_Docs that could serve as evidence for the assumed correct answer.\n"
        "- Do NOT require lexical overlap with the original query; prioritize abstract evidence.\n"
        "- Evidence forms may include (not exhaustive): theory/mechanism, entity/fact, analogy/example, method/metric, canonical reference.\n"
        "Output JSON only:\n"
        "{\n"
        "  \"action\": \"exploit\",\n"
        "  \"Plan\": \"short reasoning\",\n"
        "  \"Possible_Answer_Docs\": {\n"
        "    \"Theory\": \"...\",\n"
        "    \"Entity\": \"...\",\n"
        "    \"Example\": \"...\",\n"
        "    \"Other\": \"...\"\n"
        "  }\n"
        "}\n\n"
        "Original Query:\n{original_query}\n\n"
        "Previous Rewritten Query:\n{previous_rewrite}\n\n"
        "Leaf Evidence:\n{leaf_descs}\n\n"
        "Branch Context:\n{branch_descs}\n"
    ),
    "round3_action_v1_exploit": (
        "You are rewriting a search query for reasoning-intensive retrieval.\n\n"
        "Goal:\n"
        "- You MUST choose EXPLOIT.\n"
        "- Refine the current hypothesis while staying abstract.\n\n"
        "Task:\n"
        "- Write a short Plan that follows these steps:\n"
        "  1. Identify the user's intent and answer type.\n"
        "  2. Abstraction: infer which academic terms, theories, models, or canonical methods would be cited in a correct answer.\n"
        "  3. Use evidence to refine the current hypothesis and **use key terms from evidence as anchors**.\n"
        "  4. Ensure every generated document is abstract evidence (NOT surface-level restatements).\n"
        "- Produce 3-5 distinct Possible_Answer_Docs that could serve as evidence for the assumed correct answer.\n"
        "- Do NOT require lexical overlap with the original query; prioritize abstract evidence.\n"
        "- Evidence forms may include (not exhaustive): theory/mechanism, entity/fact, analogy/example, method/metric, canonical reference.\n"
        "Output JSON only:\n"
        "{\n"
        "  \"action\": \"exploit\",\n"
        "  \"Plan\": \"short reasoning\",\n"
        "  \"Possible_Answer_Docs\": {\n"
        "    \"Theory\": \"...\",\n"
        "    \"Entity\": \"...\",\n"
        "    \"Example\": \"...\",\n"
        "    \"Other\": \"...\"\n"
        "  }\n"
        "}\n\n"
        "Original Query:\n{original_query}\n\n"
        "Previous Rewritten Query:\n{previous_rewrite}\n\n"
        "Leaf Evidence:\n{leaf_descs}\n\n"
        "Branch Context:\n{branch_descs}\n"
    ),
    "round3_action_v1_explore": (
        "You are rewriting a search query for reasoning-intensive retrieval.\n\n"
        "Goal:\n"
        "- You MUST choose EXPLORE.\n"
        "- Pivot to a new abstract hypothesis and avoid the failed direction.\n\n"
        "Task:\n"
        "- Write a short Plan that follows these steps:\n"
        "  1. Identify the user's intent and answer type.\n"
        "  2. Abstraction: infer which academic terms, theories, models, or canonical methods would be cited in a correct answer.\n"
        "  3. Treat the previous rewrite as a negative constraint and propose a different abstract direction.\n"
        "  4. Ensure every generated document is abstract evidence (NOT surface-level restatements).\n"
        "- Produce 3-5 distinct Possible_Answer_Docs that could serve as evidence for the assumed correct answer.\n"
        "- Do NOT require lexical overlap with the original query; prioritize abstract evidence.\n"
        "- Evidence forms may include (not exhaustive): theory/mechanism, entity/fact, analogy/example, method/metric, canonical reference.\n"
        "Output JSON only:\n"
        "{\n"
        "  \"action\": \"explore\",\n"
        "  \"Plan\": \"short reasoning\",\n"
        "  \"Possible_Answer_Docs\": {\n"
        "    \"Theory\": \"...\",\n"
        "    \"Entity\": \"...\",\n"
        "    \"Example\": \"...\",\n"
        "    \"Other\": \"...\"\n"
        "  }\n"
        "}\n\n"
        "Original Query:\n{original_query}\n\n"
        "Previous Rewritten Query:\n{previous_rewrite}\n\n"
        "Leaf Evidence:\n{leaf_descs}\n\n"
        "Branch Context:\n{branch_descs}\n"
    ),
    "round3_action_v2": (
        "You are rewriting a search query for reasoning-intensive retrieval.\n\n"
        "Goal:\n"
        "- Decide whether to stay local (EXPLOIT), keep the current rewrite as-is (HOLD), or change direction (EXPLORE).\n"
        "- If evidence supports the current hypothesis but no update is needed, choose HOLD.\n"
        "- If evidence supports the current hypothesis and you can refine it, choose EXPLOIT.\n"
        "- If evidence is missing, contradictory, or points elsewhere, choose EXPLORE.\n\n"
        "Task:\n"
        "- Write a short Plan that follows these steps:\n"
        "  1. Identify the user's intent and answer type.\n"
        "  2. Abstraction: infer which academic terms, theories, models, or canonical methods would be cited in a correct answer.\n"
        "  3. Verification: compare the evidence against your current hypothesis.\n"
        "     - If supported and no change needed -> HOLD.\n"
        "     - If supported and refinement is needed -> EXPLOIT.\n"
        "     - If missing/contradictory -> EXPLORE and treat the previous rewrite as a negative constraint.\n"
        "  4. Ensure every generated document is abstract evidence (NOT surface-level restatements).\n"
        "- Action-specific behavior:\n"
        "  - HOLD: Keep the previous rewrite exactly; do not shift to a new terms.\n"
        "  - EXPLOIT: Refine the current hypothesis and **use key terms from evidence as anchors**, while staying abstract.\n"
        "  - EXPLORE: Pivot to a new abstract hypothesis (different theory/framework/entity name), avoiding the failed direction.\n"
        "- Produce 3-5 distinct Possible_Answer_Docs that could serve as evidence for the assumed correct answer.\n"
        "- Do NOT require lexical overlap with the original query; prioritize abstract evidence.\n"
        "- Evidence forms may include (not exhaustive): theory/mechanism, entity/fact, analogy/example, method/metric, canonical reference.\n"
        "Output JSON only:\n"
        "{\n"
        "  \"action\": \"hold\",\n"
        "  \"Plan\": \"short reasoning\",\n"
        "  \"Possible_Answer_Docs\": {\n"
        "    \"Theory\": \"...\",\n"
        "    \"Entity\": \"...\",\n"
        "    \"Example\": \"...\",\n"
        "    \"Other\": \"...\"\n"
        "  }\n"
        "}\n\n"
        "Original Query:\n{original_query}\n\n"
        "Previous Rewritten Query:\n{previous_rewrite}\n\n"
        "Leaf Evidence:\n{leaf_descs}\n\n"
        "Branch Context:\n{branch_descs}\n"
    ),
    "round3_action_v3": (
        "You are rewriting a search query for reasoning-intensive retrieval.\n\n"
        "Goal:\n"
        "- Decide whether to stay local (EXPLOIT), delete one or more abstraction categories (PRUNE), or change direction (EXPLORE).\n"
        "- If evidence supports the current hypothesis and you can refine it, choose EXPLOIT.\n"
        "- If a category is irrelevant or noisy, choose PRUNE and remove that category from Possible_Answer_Docs.\n"
        "- If evidence is missing, contradictory, or points elsewhere, choose EXPLORE.\n\n"
        "Task:\n"
        "- Write a short Plan that follows these steps:\n"
        "  1. Identify the user's intent and answer type.\n"
        "  2. Abstraction: infer which academic terms, theories, models, or canonical methods would be cited in a correct answer.\n"
        "  3. Verification: compare the evidence against your current hypothesis.\n"
        "     - If supported -> EXPLOIT.\n"
        "     - If missing/contradictory -> EXPLORE and treat the previous rewrite as a negative constraint.\n"
        "     - If a category adds noise -> PRUNE that category.\n"
        "  4. Ensure every generated document is abstract evidence (NOT surface-level restatements).\n"
        "- Action-specific behavior:\n"
        "  - EXPLOIT: Refine the current hypothesis and **use key terms from evidence as anchors**, while staying abstract.\n"
        "  - EXPLORE: Pivot to a new abstract hypothesis (different theory/framework/entity name), avoiding the failed direction.\n"
        "  - PRUNE: Remove one or more abstraction categories by omitting them or leaving them empty in Possible_Answer_Docs.\n"
        "- Produce 3-5 distinct Possible_Answer_Docs that could serve as evidence for the assumed correct answer.\n"
        "- Do NOT require lexical overlap with the original query; prioritize abstract evidence.\n"
        "- Evidence forms may include (not exhaustive): theory/mechanism, entity/fact, analogy/example, method/metric, canonical reference.\n"
        "Output JSON only:\n"
        "{\n"
        "  \"action\": \"prune\",\n"
        "  \"Plan\": \"short reasoning\",\n"
        "  \"Possible_Answer_Docs\": {\n"
        "    \"Theory\": \"...\",\n"
        "    \"Entity\": \"...\",\n"
        "    \"Example\": \"...\",\n"
        "    \"Other\": \"...\"\n"
        "  }\n"
        "}\n\n"
        "Original Query:\n{original_query}\n\n"
        "Previous Rewritten Query:\n{previous_rewrite}\n\n"
        "Leaf Evidence:\n{leaf_descs}\n\n"
        "Branch Context:\n{branch_descs}\n"
    ),
    "round3_action_levels_v1": (
        "You are rewriting a search query for reasoning-intensive retrieval with per-category actions.\n\n"
        "Goal:\n"
        "- Decide EXPLOIT/EXPLORE/PRUNE for each abstraction category.\n"
        "- EXPLOIT if evidence supports the previous hypothesis for that category.\n"
        "- EXPLORE if evidence is missing/contradictory or points elsewhere.\n"
        "- PRUNE if the category is not useful for this query.\n\n"
        "Inputs:\n"
        "- Original Query\n"
        "- Previous Possible Answer Docs (per category)\n"
        "- Leaf Evidence (retrieved summaries)\n"
        "- Branch Context (optional)\n\n"
        "Task:\n"
        "- Write a short Plan following these steps:\n"
        "  1. Identify user intent and answer type.\n"
        "  2. Abstraction: infer which theories, models, entities, or canonical methods would be cited in a correct answer.\n"
        "  3. Verification: compare evidence against previous hypotheses per category.\n"
        "  4. Ensure every generated document is abstract evidence (not surface-level restatement).\n"
        "- Action-specific behavior per category:\n"
        "  - EXPLOIT: Refine the previous doc and **use key terms from evidence** while staying abstract.\n"
        "  - EXPLORE: Pivot to a new abstract hypothesis (different theory/framework/entity), avoiding the failed direction.\n"
        "  - PRUNE: Remove this category.\n"
        "- Produce 3-5 distinct Possible_Answer_Docs total (across categories).\n"
        "- Do NOT require lexical overlap with the original query; prioritize abstract evidence.\n\n"
        "Output JSON only:\n"
        "{\n"
        "  \"Plan\": \"short reasoning\",\n"
        "  \"Actions\": {\n"
        "    \"Theory\": \"EXPLOIT\",\n"
        "    \"Entity\": \"EXPLORE\",\n"
        "    \"Example\": \"PRUNE\",\n"
        "    \"Other\": \"EXPLOIT\"\n"
        "  },\n"
        "  \"Possible_Answer_Docs\": {\n"
        "    \"Theory\": \"...\",\n"
        "    \"Entity\": \"...\",\n"
        "    \"Example\": \"...\",\n"
        "    \"Other\": \"...\"\n"
        "  }\n"
        "}\n\n"
        "Original Query:\n{original_query}\n\n"
        "Previous Possible Answer Docs:\n{previous_docs}\n\n"
        "Previous Rewritten Query:\n{previous_rewrite}\n\n"
        "Leaf Evidence:\n{leaf_descs}\n\n"
        "Branch Context:\n{branch_descs}\n"
    ),
    "stepback_json": (
        "Given a query, the provided passages (most of which may be incorrect or irrelevant), and the previous round's answer (if any), "
        "your goal is to analyze the candidate passages retrieved for the query, "
        "identify which of them are strictly relevant to the user's true intent, "
        "and then refine the set of answer documents accordingly. "
        "Each refined document must be strictly relevant: it should either contain the "
        "exact answer or provide abstractive-level theory, evidence, or background that is "
        "necessary for that answer.\n\n"
        "Output Format:\n"
        "You must output a single JSON object with the following keys:\n"
        "- \"Plan\": A detailed string where you analyze availability of information and plan your selection. Follow the planning steps below.\n"
        "- \"Possible_Answer_Docs\": A JSON object with keys as document types (\"Theory\", \"Entity\", \"Example\", \"Other\") and values as strings representing distinct answer documents.\n\n"
        "Planning Steps (for the \"Plan\" field):\n"
        "1. Precisely identify what information the user is seeking "
        "(e.g., why / how / what / what is this called / which part) and what type of "
        "answer would satisfy them (e.g., concept name, theory, explanation, concrete "
        "entity, worked example).\n"
        "2. If the query is explicitly or implicitly asking 'what is this called?', "
        "generate diverse hypotheses for the name of the phenomenon, object, or concept.\n"
        "3. Abstraction: infer which academic terms, scientific theories, mathematical "
        "models, canonical methods, or standard resources lie behind this question and "
        "would be cited in a strictly correct answer.\n"
        "4. Consider alternative ways the answer might be supported: a direct definition, "
        "background theory, canonical examples, or reference websites.\n"
        "5. For causal or explanatory questions (why/how), identify multiple theoretical "
        "frameworks that offer different explanations, including mainstream, alternative, "
        "and controversial perspectives if mentioned in the query.\n"
        "6. For each Candidate Passage, judge which part of the user's query does the passage address. Then, "
        "identify the common wrong direction (topic, answer type, or framing) suggested by "
        "the Candidate Passages, and treat this as a negative constraint. Plan how to "
        "refine the answer documents to avoid that direction and instead explore a different, more "
        "plausible interpretation aligned with the user's wording.\n"
        "Ensure that every generated document contributes directly and strictly to such an "
        "answer (no loosely related or merely interesting content).\n\n"
        "Final Answer Requirements (for the \"Answer documents\" field):\n"
        "Include 3-5 distinct answer-document entries:\n"
        "- 1: Concept/Theory-focused (academic terms, principles, models).\n"
        "- 2: Entity/Fact-focused (specific names, objects, or concrete facts).\n"
        "- 3: Broad Context/Evidence-focused (surveys, experiments, or background "
        "that support the answer).\n"
        "- Document 4â€“5 (optional): Any additional strictly relevant documents that provide "
        "alternative but correct perspectives or examples.\n\n"
        "Query: {query}\n\n"
        "Provide the JSON output.\n"
        "Example Output Format:\n"
        "```json\n"
        "{\n"
        "  \"Plan\": \"Detailed plan applying Step 6 logic...\",\n"
        "  \"Possible_Answer_Docs\": {\n"
        "    \"Theory\": \"...text...\",\n"
        "    \"Entity\": \"...text...\",\n"
        "    \"Example\": \"...text...\",\n"
        "    \"Other\": \"...text...\"\n"
        "  }\n"
        "}\n"
        "```\n"
    ),
    "agent_executor_v1": (
        "You are rewriting a search query for reasoning-intensive retrieval.\n\n"
        "Key idea:\n"
        "- Assume you already have the correct answer to the user query.\n"
        "- The documents we want are the ones that would be used as core evidence or justification for that answer.\n"
        "- These evidence documents are often more abstract than the surface query.\n\n"
        "Task:\n"
        "- First, write a 1-2 sentence Plan that states the user's core intent and what kind of evidence would justify the answer.\n"
        "- Use the context summaries only as hints (not ground truth).\n"
        "- Produce 2-5 distinct Possible_Answer_Docs that could serve as evidence for the assumed correct answer.\n"
        "\t- Propose multiple plausible theories/mechanisms.\n"
        "\t- Propose concrete entities/phenomena.\n"
        "\t- Propose examples/analogies.\n"
        "\t- Propose other possible useful documents.\n"
        "**Output Format:**\n"
        "Output a single JSON object. The values will be concatenated to form the next query.\n"
        "```json\n"
        "{\n"
        "  \"Plan\": \"...\",\n"
        "  \"Possible_Answer_Docs\": {\n"
        "    \"Theory\": \"...text...\",\n"
        "    \"Entity\": \"...text...\",\n"
        "    \"Example\": \"...text...\",\n"
        "    \"Other\": \"...text...\"\n"
        "  }\n"
        "}\n"
        "```\n\n"
        "Original Query:\n{original_query}\n\n"
        "Previous Rewritten Query:\n{previous_rewrite}\n\n"
        "Context Summaries:\n{gate_descs}\n"
    ),
    "baseline_round3_action_v1": (
        "You are rewriting a search query for reasoning-intensive retrieval.\n\n"
        "Goal:\n"
        "- Decide whether to stay local (EXPLOIT) or change direction (EXPLORE).\n"
        "- If evidence supports the current hypothesis (previous rewrite), choose EXPLOIT.\n"
        "- If evidence is missing, contradictory, or points elsewhere, choose EXPLORE.\n\n"
        "Task:\n"
        "- Write a short Plan that follows these steps:\n"
        "  1. Identify the user's intent and answer type.\n"
        "  2. Abstraction: infer which academic terms, theories, models, or canonical methods would be cited in a correct answer.\n"
        "  3. Verification: compare the evidence against your current hypothesis.\n"
        "     - If supported -> EXPLOIT.\n"
        "     - If missing/contradictory -> EXPLORE and treat the previous rewrite as a negative constraint.\n"
        "  4. Ensure every generated document is abstract evidence (NOT surface-level restatements).\n"
        "- Action-specific behavior:\n"
        "  - EXPLOIT: Refine the current hypothesis and **use key terms from evidence as anchors**, while staying abstract.\n"
        "  - EXPLORE: Pivot to a new abstract hypothesis (different theory/framework/entity name), avoiding the failed direction.\n"
        "- Produce 3-5 distinct Possible_Answer_Docs that could serve as evidence for the assumed correct answer.\n"
        "- Do NOT require lexical overlap with the original query; prioritize abstract evidence.\n"
        "- Evidence forms may include (not exhaustive): theory/mechanism, entity/fact, analogy/example, method/metric, canonical reference.\n"
        "Output JSON only:\n"
        "{\n"
        "  \"action\": \"exploit\",\n"
        "  \"Plan\": \"short reasoning\",\n"
        "  \"Possible_Answer_Docs\": {\n"
        "    \"Theory\": \"...\",\n"
        "    \"Entity\": \"...\",\n"
        "    \"Example\": \"...\",\n"
        "    \"Other\": \"...\"\n"
        "  }\n"
        "}\n\n"
        "Original Query:\n{original_query}\n\n"
        "Previous Rewritten Query:\n{previous_rewrite}\n\n"
        "Context Summaries:\n{gate_descs}\n"
    ),
    "thinkqe": (
        "Given a query, the provided passages (most of which may be incorrect or irrelevant), and the previous rewritten query, "
        "identify helpful information from the passages and refine the prior query.\n"
        "- Ensure the output directly improves retrieval for the original query.\n"
        "- Use your own knowledge when needed; do not blindly copy passages.\n"
        "- Output ONLY the rewritten query text.\n\n"
        "Original Query:\n{original_query}\n\n"
        "Previous Rewritten Query:\n{previous_rewrite}\n\n"
        "Context Summaries:\n{gate_descs}\n"
    )
}

SCHEMA_PROMPT_TEMPLATE = (
    "You are defining abstraction categories for query rewriting.\n\n"
    "Given:\n"
    "- Original query\n"
    "- Document descriptions (query-agnostic summaries)\n\n"
    "Task:\n"
    "- Your task is not to answer the query.\n"
    "- Abstraction: infer which TYPE of information e.g., terms, scientific theories, mathematical "
    "models, canonical methods, or standard resources lie behind and "
    "would be cited in an answer to the question.\n"
    "- Scan corpus: use the document descriptions as hints for which kind of documents are in the corpus.\n\n"
    "- Select 3-5 abstraction category labels from document descriptions.\n"
    "- Categories must be 1-4 words, short, and non-overlapping.\n"
    "- Avoid specific terms; focus on abstract types of information.\n"
    "- Light examples of labels: "
    "theory, key entity, mathematical model, empirical/analogical evidence, example\n\n"
    "Output JSON only:\n"
    "{\n"
    "  \"schema_labels\": [\"...\", \"...\", \"...\"]\n"
    "}\n\n"
    "Original Query:\n{original_query}\n\n"
    "Document Descriptions:\n{branch_descs}\n"
)
