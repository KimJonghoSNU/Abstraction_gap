REWRITE_PROMPT_TEMPLATES = {
    "gate_rewrite_v1": (
        "You are rewriting a search query for reasoning-intensive retrieval.\n\n"
        "Key idea:\n"
        "- Assume you already have the correct answer to the user query.\n"
        "- The documents we want are the ones that would be used as core evidence or justification for that answer.\n"
        "- These evidence documents are often more abstract than the surface query.\n\n"
        "Task:\n"
        "- First, write a 1-2 sentence Plan that states the user's core intent and what kind of evidence would justify the answer.\n"
        "- Use the context summaries only as hints (not ground truth).\n"
        "- Produce 2-5 distinct Possible_Answer_Docs that could serve as evidence for the assumed correct answer.\n"
        "- Avoid near-duplicates; keep each item short and retrieval-friendly.\n"
        "- Evidence forms may include (not exhaustive): theory/mechanism, entity/fact, analogy/example, method/metric, canonical reference.\n\n"
        "Output JSON only:\n"
        "{\n"
        "  \"Plan\": \"short reasoning\",\n"
        "  \"Possible_Answer_Docs\": [\n"
        "    \"...\",\n"
        "    \"...\",\n"
        "    \"...\"\n"
        "  ]\n"
        "}\n\n"
        "Original Query:\n{original_query}\n\n"
        "Previous Rewritten Query:\n{previous_rewrite}\n\n"
        "Context Summaries:\n{gate_descs}\n"
    ),
    "gate_rewrite_schema_v1": (
        "You are rewriting a search query for reasoning-intensive retrieval.\n\n"
        "Key idea:\n"
        "- Assume you already have the correct answer to the user query.\n"
        "- The documents we want are the ones that would be used as core evidence or justification for that answer.\n"
        "- These evidence documents are often more abstract than the surface query.\n\n"
        "Task:\n"
        "- First, write a 1-2 sentence Plan that states the user's core intent, answer, and what kind of evidence would justify the answer.\n"
        "- Produce 2-5 distinct Possible_Answer_Docs, guided by the given categories.\n"
        "Output JSON only:\n"
        "{\n"
        "  \"Plan\": \"short reasoning\",\n"
        "  \"Possible_Answer_Docs\": {\n"
        "{schema_kv_template}\n"
        "  }\n"
        "}\n\n"
        "Original Query:\n{original_query}\n\n"
        "Previous Rewritten Query:\n{previous_rewrite}\n\n"
        "Context Summaries:\n{gate_descs}\n"
    ),
    "stepback_json": (
        "Given a query, the provided passages (most of which may be incorrect or irrelevant), and the previous round's answer (if any), "
        "your goal is to analyze the candidate passages retrieved for the query, "
        "identify which of them are strictly relevant to the user's true intent, "
        "and then refine the set of answer documents accordingly. "
        "Each refined document must be strictly relevant: it should either contain the "
        "exact answer or provide abstractive-level theory, evidence, or background that is "
        "necessary for that answer.\n\n"
        "Output Format:\n"
        "You must output a single JSON object with the following keys:\n"
        "- \"Plan\": A detailed string where you analyze availability of information and plan your selection. Follow the planning steps below.\n"
        "- \"Possible_Answer_Docs\": A JSON object with keys as document types (\"Theory\", \"Entity\", \"Example\", \"Other\") and values as strings representing distinct answer documents.\n\n"
        "Planning Steps (for the \"Plan\" field):\n"
        "1. Precisely identify what information the user is seeking "
        "(e.g., why / how / what / what is this called / which part) and what type of "
        "answer would satisfy them (e.g., concept name, theory, explanation, concrete "
        "entity, worked example).\n"
        "2. If the query is explicitly or implicitly asking 'what is this called?', "
        "generate diverse hypotheses for the name of the phenomenon, object, or concept.\n"
        "3. Abstraction: infer which academic terms, scientific theories, mathematical "
        "models, canonical methods, or standard resources lie behind this question and "
        "would be cited in a strictly correct answer.\n"
        "4. Consider alternative ways the answer might be supported: a direct definition, "
        "background theory, canonical examples, or reference websites.\n"
        "5. For causal or explanatory questions (why/how), identify multiple theoretical "
        "frameworks that offer different explanations, including mainstream, alternative, "
        "and controversial perspectives if mentioned in the query.\n"
        "6. For each Candidate Passage, judge which part of the user's query does the passage address. Then, "
        "identify the common wrong direction (topic, answer type, or framing) suggested by "
        "the Candidate Passages, and treat this as a negative constraint. Plan how to "
        "refine the answer documents to avoid that direction and instead explore a different, more "
        "plausible interpretation aligned with the user's wording.\n"
        "Ensure that every generated document contributes directly and strictly to such an "
        "answer (no loosely related or merely interesting content).\n\n"
        "Final Answer Requirements (for the \"Answer documents\" field):\n"
        "Include 3-5 distinct answer-document entries:\n"
        "- 1: Concept/Theory-focused (academic terms, principles, models).\n"
        "- 2: Entity/Fact-focused (specific names, objects, or concrete facts).\n"
        "- 3: Broad Context/Evidence-focused (surveys, experiments, or background "
        "that support the answer).\n"
        "- Document 4â€“5 (optional): Any additional strictly relevant documents that provide "
        "alternative but correct perspectives or examples.\n\n"
        "Query: {query}\n\n"
        "Provide the JSON output.\n"
        "Example Output Format:\n"
        "```json\n"
        "{\n"
        "  \"Plan\": \"Detailed plan applying Step 6 logic...\",\n"
        "  \"Possible_Answer_Docs\": {\n"
        "    \"Theory\": \"...text...\",\n"
        "    \"Entity\": \"...text...\",\n"
        "    \"Example\": \"...text...\",\n"
        "    \"Other\": \"...text...\"\n"
        "  }\n"
        "}\n"
        "```\n"
    ),
}

SCHEMA_PROMPT_TEMPLATE = (
    "You are defining abstraction categories for query rewriting.\n\n"
    "Given:\n"
    "- Original query\n"
    "- Document descriptions (query-agnostic summaries)\n\n"
    "Task:\n"
    "- Your task is not to answer the query.\n"
    "- Abstraction: infer which TYPE of information e.g., terms, scientific theories, mathematical "
    "models, canonical methods, or standard resources lie behind and "
    "would be cited in an answer to the question.\n"
    "- Scan corpus: use the document descriptions as hints for which kind of documents are in the corpus.\n\n"
    "- Select 3-5 abstraction category labels from document descriptions.\n"
    "- Categories must be 1-4 words, short, and non-overlapping.\n"
    "- Avoid specific terms; focus on abstract types of information.\n"
    "- Light examples of labels: "
    "theory, key entity, mathematical model, empirical/analogical evidence, example\n\n"
    "Output JSON only:\n"
    "{\n"
    "  \"schema_labels\": [\"...\", \"...\", \"...\"]\n"
    "}\n\n"
    "Original Query:\n{original_query}\n\n"
    "Document Descriptions:\n{branch_descs}\n"
)
