QE_PROMPT_TEMPLATES = {
    # Bridge-focused: explicitly try to jump abstraction levels without assuming a fixed taxonomy.
    "bridge_v1": (
        "You are helping retrieval for a reasoning-intensive question where the answer may live at a different abstraction level.\n"
        "Produce a expanded retrieval query that includes:\n"
        "- the original query intent,\n"
        "- possible underlying mechanisms / theories / examples / canonical terms,\n"
        "- alternative formulations and related concepts that could bridge to the correct topic cluster.\n"
        "Output ONLY the expanded query text.\n\n"
        "User query:\n{query}\n"
    ),
    # Adapted from previous/shortcut_reranker/utils/agent_prompts.py (executor-style).
    "agent_executor_v1": (
        "Your goal is to generate the next set of 'Possible Answer Documents' (Search Queries).\n"
        "No prior retrieved docs are given, so propose diverse hypotheses that could bridge abstraction gaps.\n\n"
        "**Thinking Process (Execute inside 'Plan'):**\n"
        "1. Identify User Intent & Answer Type.\n"
        "2. Propose multiple plausible theories/mechanisms.\n"
        "3. Propose concrete entities/phenomena.\n"
        "4. Propose examples/analogies.\n"
        "5. Ensure each item is directly useful for retrieval.\n\n"
        "**Output Format:**\n"
        "Output a single JSON object. The values will be concatenated to form the next query.\n"
        "```json\n"
        "{\n"
        "  \"Plan\": \"...\",\n"
        "  \"Possible_Answer_Docs\": {\n"
        "    \"Theory\": \"...text...\",\n"
        "    \"Entity\": \"...text...\",\n"
        "    \"Example\": \"...text...\",\n"
        "    \"Other\": \"...text...\"\n"
        "  }\n"
        "}\n"
        "```\n\n"
        "Original Query: {query}\n"
    ),
    "pre_flat_rewrite_v1": (
        "You are rewriting a search query for reasoning-intensive retrieval.\n\n"
        "Key idea:\n"
        "- Assume you already have the correct answer to the user query.\n"
        "- The documents we want are the ones that would be used as core evidence or justification for that answer.\n"
        "- These evidence documents are often more abstract than the surface query.\n\n"
        "Task:\n"
        "- First, write a 1-2 sentence Plan that states the user's core intent and what kind of evidence would justify the answer.\n"
        "- Produce 2-5 distinct Possible_Answer_Docs that could serve as evidence for the assumed correct answer.\n"
        "- Avoid near-duplicates; keep each item short and retrieval-friendly.\n"
        "- Evidence forms may include (not exhaustive): theory/mechanism, entity/fact, analogy/example, method/metric, canonical reference.\n\n"
        "Output JSON only:\n"
        "{\n"
        "  \"Plan\": \"short reasoning\",\n"
        "  \"Possible_Answer_Docs\": [\n"
        "    \"...\",\n"
        "    \"...\",\n"
        "    \"...\"\n"
        "  ]\n"
        "}\n\n"
        "Original Query:\n{query}\n"
    ),
}
