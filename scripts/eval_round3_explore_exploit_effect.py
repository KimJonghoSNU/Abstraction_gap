import argparse
import json
import os
import pickle as pkl
from collections import defaultdict
from typing import Dict, List, Sequence, Tuple

import numpy as np


Path = Tuple[int, ...]


def _get_iter_records(sample: Dict) -> List[Dict]:
    recs = list(sample.get("iter_records", []))
    recs.sort(key=lambda r: int(r.get("iter", -1)))
    return recs


def _to_path_list(paths_obj: Sequence[Sequence[int]]) -> List[Path]:
    return [tuple(int(x) for x in p) for p in paths_obj]


def _get_oracle_action_topk(rec: Dict, action_tag: str, k: int) -> List[Path]:
    d = rec.get("oracle_action_anchor_top10") or {}
    if not isinstance(d, dict):
        return []
    action_paths = d.get(action_tag, [])
    if not action_paths:
        return []
    return _to_path_list(action_paths[:k])


def _get_chosen_action(rec: Dict) -> str:
    action = str(rec.get("oracle_action_choice") or rec.get("action") or "").strip().lower()
    return action if action else "unknown"


def _topk_set(paths: Sequence[Path], k: int) -> set:
    return set(paths[:k])


def _overlap_count_at_k(a_paths: Sequence[Path], b_paths: Sequence[Path], k: int) -> int:
    return int(len(_topk_set(a_paths, k) & _topk_set(b_paths, k)))


def _jaccard_at_k(a_paths: Sequence[Path], b_paths: Sequence[Path], k: int) -> float:
    a = _topk_set(a_paths, k)
    b = _topk_set(b_paths, k)
    if not a and not b:
        return 1.0
    union = a | b
    if not union:
        return 1.0
    return float(len(a & b)) / float(len(union))


def _mean(values: List[float]) -> float:
    if not values:
        return 0.0
    return float(np.mean(np.array(values, dtype=np.float32)))


def _tokenize(text: str) -> List[str]:
    return [t for t in (text or "").lower().strip().split() if t]


def _token_jaccard(a_text: str, b_text: str) -> float:
    a = set(_tokenize(a_text))
    b = set(_tokenize(b_text))
    if not a and not b:
        return 1.0
    union = a | b
    if not union:
        return 1.0
    return float(len(a & b)) / float(len(union))


def _build_report_for_file(
    *,
    eval_pkl_path: str,
    eval_k: int,
) -> Dict:
    sample_dicts = pkl.load(open(eval_pkl_path, "rb"))
    metric_by_selected_action: Dict[str, Dict[str, List[float]]] = defaultdict(lambda: defaultdict(list))
    count_by_selected_action: Dict[str, int] = defaultdict(int)
    counterfactual_metrics: Dict[str, List[float]] = defaultdict(list)
    chosen_better_flags: List[float] = []

    for sample in sample_dicts:
        original_query = str(sample.get("original_query") or "").strip()
        if not original_query:
            continue

        recs = _get_iter_records(sample)
        if not recs:
            continue

        iter0_chosen_paths: List[Path] | None = None
        prev_chosen_paths: List[Path] | None = None

        for rec in recs:
            action = _get_chosen_action(rec)
            if action not in {"explore", "exploit"}:
                continue

            explore_paths = _get_oracle_action_topk(rec, "explore", eval_k)
            exploit_paths = _get_oracle_action_topk(rec, "exploit", eval_k)
            if not explore_paths or not exploit_paths:
                continue

            chosen_paths = explore_paths if action == "explore" else exploit_paths
            query_t = str(rec.get("query_t") or "").strip()
            if not query_t:
                query_t = original_query

            count_by_selected_action[action] += 1
            metric_by_selected_action[action]["QueryTokenJaccardToOriginal"].append(
                _token_jaccard(query_t, original_query)
            )

            if iter0_chosen_paths is None:
                iter0_chosen_paths = chosen_paths

            if prev_chosen_paths is not None:
                overlap_prev = _overlap_count_at_k(chosen_paths, prev_chosen_paths, eval_k)
                metric_by_selected_action[action][f"RetainedCountToPrev@{eval_k}"].append(float(overlap_prev))
                metric_by_selected_action[action][f"RetainedRateToPrev@{eval_k}"].append(
                    float(overlap_prev) / float(max(1, eval_k))
                )
                metric_by_selected_action[action][f"JaccardToPrev@{eval_k}"].append(
                    _jaccard_at_k(chosen_paths, prev_chosen_paths, eval_k)
                )

            if iter0_chosen_paths is not None:
                overlap_iter0 = _overlap_count_at_k(chosen_paths, iter0_chosen_paths, eval_k)
                metric_by_selected_action[action][f"RetainedCountToIter0@{eval_k}"].append(float(overlap_iter0))
                metric_by_selected_action[action][f"RetainedRateToIter0@{eval_k}"].append(
                    float(overlap_iter0) / float(max(1, eval_k))
                )
                metric_by_selected_action[action][f"JaccardToIter0@{eval_k}"].append(
                    _jaccard_at_k(chosen_paths, iter0_chosen_paths, eval_k)
                )

            # Intent: counterfactual context drift between explore and exploit at the same iteration.
            overlap_ex = _overlap_count_at_k(explore_paths, exploit_paths, eval_k)
            counterfactual_metrics[f"ExploreVsExploit_RetainedCount@{eval_k}"].append(float(overlap_ex))
            counterfactual_metrics[f"ExploreVsExploit_RetainedRate@{eval_k}"].append(
                float(overlap_ex) / float(max(1, eval_k))
            )
            counterfactual_metrics[f"ExploreVsExploit_Jaccard@{eval_k}"].append(
                _jaccard_at_k(explore_paths, exploit_paths, eval_k)
            )

            explore_ndcg = rec.get("oracle_explore_ndcg@10")
            exploit_ndcg = rec.get("oracle_exploit_ndcg@10")
            if explore_ndcg is not None and exploit_ndcg is not None:
                explore_ndcg_f = float(explore_ndcg)
                exploit_ndcg_f = float(exploit_ndcg)
                counterfactual_metrics["ExploreMinusExploit_nDCG@10_candidate"].append(
                    explore_ndcg_f - exploit_ndcg_f
                )
                if action == "explore":
                    chosen_better_flags.append(float(explore_ndcg_f >= exploit_ndcg_f))
                elif action == "exploit":
                    chosen_better_flags.append(float(exploit_ndcg_f >= explore_ndcg_f))

            prev_chosen_paths = chosen_paths

    summary_by_action: Dict[str, Dict[str, float]] = {}
    for action, metric_lists in metric_by_selected_action.items():
        summary_by_action[action] = {
            "count": int(count_by_selected_action[action]),
        }
        for metric_name, values in metric_lists.items():
            summary_by_action[action][metric_name] = _mean(values)

    counterfactual_summary = {metric_name: _mean(values) for metric_name, values in counterfactual_metrics.items()}
    counterfactual_summary["n_records"] = int(sum(count_by_selected_action.values()))
    if chosen_better_flags:
        counterfactual_summary["ChosenActionIsBetterCandidateRate"] = _mean(chosen_better_flags) * 100.0

    def get_metric(action: str, metric: str) -> float:
        return float(summary_by_action.get(action, {}).get(metric, 0.0))

    compare = {
        "count_explore": int(count_by_selected_action.get("explore", 0)),
        "count_exploit": int(count_by_selected_action.get("exploit", 0)),
        "ExploreChosenRate": (
            100.0 * float(count_by_selected_action.get("explore", 0))
            / float(max(1, sum(count_by_selected_action.values())))
        ),
        "Delta_QueryTokenJaccardToOriginal_explore_minus_exploit": (
            get_metric("explore", "QueryTokenJaccardToOriginal")
            - get_metric("exploit", "QueryTokenJaccardToOriginal")
        ),
        f"Delta_ChosenJaccardToPrev@{eval_k}_explore_minus_exploit": (
            get_metric("explore", f"JaccardToPrev@{eval_k}") - get_metric("exploit", f"JaccardToPrev@{eval_k}")
        ),
        f"Delta_ChosenRetainedCountToPrev@{eval_k}_explore_minus_exploit": (
            get_metric("explore", f"RetainedCountToPrev@{eval_k}") - get_metric("exploit", f"RetainedCountToPrev@{eval_k}")
        ),
        f"Delta_ChosenJaccardToIter0@{eval_k}_explore_minus_exploit": (
            get_metric("explore", f"JaccardToIter0@{eval_k}") - get_metric("exploit", f"JaccardToIter0@{eval_k}")
        ),
    }

    return {
        "eval_pkl_path": eval_pkl_path,
        "n_samples": int(len(sample_dicts)),
        "by_selected_action": summary_by_action,
        "counterfactual_explore_vs_exploit": counterfactual_summary,
        "explore_vs_exploit": compare,
    }


def main() -> None:
    parser = argparse.ArgumentParser()
    parser.add_argument("--eval_pkl_paths", type=str, nargs="+", required=True)
    parser.add_argument("--eval_k", type=int, default=10)
    parser.add_argument("--output_json", type=str, required=True)
    args = parser.parse_args()

    eval_pkl_paths = [os.path.abspath(p) for p in args.eval_pkl_paths]
    for eval_pkl_path in eval_pkl_paths:
        if not os.path.exists(eval_pkl_path):
            raise FileNotFoundError(f"eval_pkl not found: {eval_pkl_path}")

    per_file_reports = [
        _build_report_for_file(
            eval_pkl_path=eval_pkl_path,
            eval_k=int(args.eval_k),
        )
        for eval_pkl_path in eval_pkl_paths
    ]

    report = {
        "meta": {
            "eval_k": int(args.eval_k),
            "note": (
                "No additional retrieval is run. Metrics are computed only from stored oracle_action_anchor_top10 "
                "and selected action/query_t in iter_records."
            ),
        },
        "reports": per_file_reports,
    }

    output_path = os.path.abspath(args.output_json)
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    print(f"Saved report: {output_path}")


if __name__ == "__main__":
    main()
