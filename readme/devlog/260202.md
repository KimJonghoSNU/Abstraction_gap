# LG에서 요구한 것들
- 기존 연구에는 어떤 Challenge?
    - 1) abstraction을 안하면 어떤 문제가 있냐
    - 2) explore를 안하면 recall이 줄어드는 방향으로 rewrite가 진행되는 경우 있는지? (locality issue에 빠져서?)
    - explore를 잘 하게 만드는 방법? 1. Exploit, explore는 다른 Path를 가져올 것이다. 
    1) corpus tree 에서 retrieval path 조절할 수 있냐? 2) corpus tree에서 하위 개념들로 exploit할 수 있냐?


Motivation: Abstraction Gap between query & document
Direction: Query rewriting w/ abstraction to write principles & evidence that support the answer
Challenge / RQs
RQ 1) LLMs do not have sufficient domain-specific knowledge to generate such principles
→ corpus-tree to represent documents in multiple abstraction level
RQ 2) Multiple evidence can be available
→ corpus-tree branch nodes to limit search space



# 선행 연구들의 challenge 1: Abstraction 안하면 문제

## 주장
Planning 기반 방법은 query 표면이 닿는 후보군 안에서 판단을 잘하는 쪽에 강점이 있다
하지만 중요 term이 query에 없는 상황에서 original query에서 벗어나지 못함

- 보일 수 있는 실험 brainstorming
    <!-- - tree한곳에 몰려있다? baseline은 몰려있고.. ours는 안몰려있다. 근데 몰려있으면 왜 안좋은거지?
    - 대안: ancestor branch hit을 측정 / 왜 recall을 직접 측정 안하고 이렇게 하냐 하면 뭐라 대답하지? -->

- (iteration 0->1 주목. 즉 Original query -> rewrite query)
    - locality를 해결하는가? rewrite하면 이전 Retrieved결과에서 얼마나 달라지는가? (topk중 몇개가 그대로 유지되는지?)
    - 제대로 벗어났는가? branch 기준으로, our prompt가 이전 branch에서 얼마나 벗어나게 해주는가?
        - `RegionHitL1@10`, `RegionHitL2@10` (+ delta)
            - gold leaf와 같은 level-1 / level-2 prefix region을 top-k가 한 번이라도 맞췄는지
    - `DeltaPrompt_Hit@10` = **+2.38**: miss 케이스를 hit으로 바꾸는 효과가 평균적으로 존재
    - `DeltaPrompt_BalancedGain` = **+10.05**: (Bad0 회복 - Good0 훼손) 관점에서도 전체적으로는 이득
    - jaccard는 miss일 때 / good일 때 delta분석용으로는 쓰지 말고, 기존 쿼리와 얼마나 안겹치게 생성하는지 분석하는 용도로만 남겨둬
    - `DeltaPrompt_Delta_RegionHitL1@10` = **+3.03pp**
    - `DeltaPrompt_Delta_RegionHitL2@10` = **+3.23pp**
    - `DeltaPrompt_Delta_RegionHitL2@10_givenPreMiss` = **+13.19pp**

### 2026-02-03 업데이트: "abstraction을 안 하면 뭐가 문제고, 무엇이 개선되나" (iter0 -> iter1)

핵심 주장

  - Abstraction 없이 rewrite하면, 초기 miss(Bad0)에서 정답 subtree로 이동하는 능력이 부족하다.
  - Abstractive rewrite는 retrieval pool을 더 적극적으로 재구성해 Bad0 탈출을 유의미하게 늘린다.

  근거 (iter0→iter1, thinkqe vs action_v1, 5개 subset 평균)

  1. 탈출 성능 개선
      - DeltaPrompt_MissToHit@10 = +13.49pp
      - DeltaPrompt_Delta_RegionHitL2@10_givenPreMiss = +13.19pp
      - 해석: “맞는 subtree로 들어가는 능력”이 실제로 올라감.
  2. 최종 retrieval 품질 개선
      - DeltaPrompt_Hit@10 = +2.38pp
      - DeltaPrompt_nDCG@10 = +5.75
      - 해석: 단순 이동이 아니라 최종 ranking 품질도 개선됨.
  3. 왜 abstraction이 필요한지 직접 증명
      - DeltaPrompt_RetainedCount@10_pre_vs_post = -1.61
      - 해석: abstractive rewrite는 top-10을 baseline보다 더 바꾸며(탐색), 그 결과 miss recovery를 만든다.
        즉 “안 바꾸면 못 찾는 케이스”가 존재함.

  균형 있는 결론 (한계 포함)

  - Good0 안정성은 과제로 남아 있음: DeltaPrompt_HarmRate@10_givenGood0 = +3.44pp.
  - 하지만 전체적으로는 DeltaPrompt_BalancedGain = +10.05로, 이득(탈출)이 손실(훼손)보다 큼.

  리뷰어용 한 문장

  - “Our results show that abstraction is not merely stylistic rewriting: it induces targeted region transitions in the corpus tree, which substantially improves miss recovery and overall ranking quality, though with a measurable robustness trade-off on already-correct cases.”





# adaptive retrieval의 필요성: abstract level 매칭 가능
- flat retrieval hit 을 측정해서, branch에 gold 가 있는지 본다 -> 이정도 기회를 우리가 놓치고 있는 거 아니니? 라고 보여줄까
즉, flat retrieval 상위에 gold leaf가 없다. 하지만 gold ancestor branch는 있다. 이건 tree가 제공하는 회복 기능

- 실험 설정
    - `anchor_local_rank=v2` run 내부에서 counterfactual 비교
        - graph-off: 같은 flat mixed ranking에서 branch를 무시하고 leaf만으로 top-10 구성
        - graph-on(v2): 같은 flat mixed ranking에서 branch를 v2 규칙(최고점 descendant leaf)으로 치환해 top-10 구성
    - 리포트 경로: `results/analysis/adaptive_recovery_within_v2/*_action_v1_within_v2_off_vs_on_iter01.json`
    - 핵심 정의
        - `Opportunity@10_givenFlatLeafMiss`: flat mixed top-10에서 leaf miss이고, 동시에 branch hit 신호가 있는 비율
        - `ConversionByGraphOn@10_givenOpportunity`: opportunity 케이스 중 graph-on(v2)에서 leaf hit으로 바뀐 비율

- 결과 (5개 subset 평균)
    - iter1 기준
        - `GraphOffLeafHit@10` = **66.51**
        - `GraphOnV2LeafHit@10` = **65.34**
        - `Delta_Hit@10_graph_on_minus_off` = **-1.17pp**
        - `GraphOff_nDCG@10` = **41.98**
        - `GraphOnV2_nDCG@10` = **41.52**
        - `Delta_nDCG@10_graph_on_minus_off` = **-0.46**
        - `FlatTopK_BranchHitAny@10` = **50.60%**
        - `Opportunity@10_givenFlatLeafMiss` = **9.45%**   
            - 분모: flat mixed top-10에서 leaf hit이 없는 샘플 수 - 공식: Opportunity_count / FlatLeafMiss_count
        - `ConversionByGraphOn@10_givenOpportunity` = **23.33%**
    - 해석
        - opportunity(회복 가능한 branch 신호)는 실제로 존재함
        - 하지만 현재 v2 치환 규칙은 opportunity를 일부만 회복하고, 평균적으로는 off 대비 성능 이득으로 연결되지 않음
        - 즉 "기회는 있는데, 현재 branch->leaf 매핑 정책이 충분히 강하지 않다"는 결론

- 결론
    - tree의 branch-level 신호 자체는 유의미하지만, 현재 v2 구현만으로는 안정적인 성능 상승을 보장하지 못함
    - 다음 단계는 opportunity 케이스에서의 conversion을 높이는 매핑 전략 개선 (예: branch scoring, descendant 선택 규칙 개선)
    - “그래프가 쓸모없다”라기보다, 현재 branch→leaf 정책이 부족한 상태.
    - 실무적으로는 지금 단계에서:
        1. 메인 결과에서는 graph-on 제거
        2. 그래프는 ablation/negative result로만 보고
        3. 나중에 강한 gating 생기면 재도입
            이 제일 깔끔해.



# explore / exploit의 효과

- Explore action을 했을 때, exploit action과 비교해서, 기존 쿼리와 similarity가 많이 차이나는가? 가져오는 context가 많이 달라지는가? 즉 프롬프팅만으로 Explore, exploit action을 취할 수 있는가?

- 분석 설정
    - 입력: `results/BRIGHT/{subset}/.../S=round3_action_oracle_ndcg_round3_action_v1.../RAO=ndcg-RRC=leaf/all_eval_sample_dicts.pkl`
    - 스크립트: `scripts/eval_round3_explore_exploit_effect.py` (추가 retrieval 없이 저장된 `oracle_action_anchor_top10` 사용)
    - 결과: `results/analysis/explore_exploit_effect/*_round3_action_oracle_action_effect.json`

- 핵심 결과 (5개 subset 평균)
    - explore 선택 비율: **12.9%** (exploit 87.1%)
    - (선택된 action 기준) QueryTokenJaccardToOriginal: explore가 exploit보다 **+0.049**
        - 해석: explore가 original query와 lexical하게 크게 멀어지지는 않음
    - (선택된 action 기준) 이전 iter 대비 context 변화
        - `Delta_ChosenJaccardToPrev@10 (explore - exploit)` = **-0.203**
        - `Delta_ChosenRetainedCountToPrev@10 (explore - exploit)` = **-1.44**
        - 해석: explore가 exploit보다 매 iter에서 context를 더 크게 바꿈
    - (동일 iter counterfactual) explore vs exploit 후보 context 차이
        - `ExploreVsExploit_Jaccard@10` = **0.595**
        - `ExploreVsExploit_RetainedCount@10` = **7.15 / 10**
        - 해석: 둘이 완전히 다른 풀은 아니고, top-10 중 약 7개는 겹침

- 메트릭 정의 (왜 두 현상이 양립 가능한지 포함)
    - `QueryTokenJaccardToOriginal`
        - 정의: `Jaccard(tokens(query_t), tokens(original_query))`
        - 의미: 쿼리 문자열이 원문과 단어 수준에서 얼마나 비슷한지
        - 주의: lexical 지표라서, 단어가 비슷해도 retrieval 결과는 크게 달라질 수 있음
    - `Delta_ChosenJaccardToPrev@10 (explore - exploit)`
        - 정의: 선택된 action의 top-10 context와 직전 iter 선택 context의 Jaccard 차이
        - 음수일수록 explore가 exploit보다 "직전 대비 더 많이 바꿈"
    - `Delta_ChosenRetainedCountToPrev@10 (explore - exploit)`
        - 정의: 선택된 action의 top-10에서 직전 iter와 겹치는 문서 수 차이
        - 음수일수록 explore가 exploit보다 유지 문서 수가 적음 (= 더 큰 이동)
    - `ExploreVsExploit_Jaccard@10`, `ExploreVsExploit_RetainedCount@10`
        - 정의: 같은 iter에서 explore 후보 top-10 vs exploit 후보 top-10의 겹침 정도
        - 의미: 두 action이 실제로 얼마나 다른 문서 풀을 제안하는지

- 왜 양립 가능한가?
    - `QueryTokenJaccardToOriginal`는 "문자열 유사도", `Jaccard/Retained`는 "검색 결과 유사도"를 봄
    - 현재 세팅은 `concat`이라 original query 토큰이 query_t에 계속 남아 lexical 유사도가 높게 나올 수 있음
    - 동시에 rewrite의 일부 신호만 바뀌어도 retriever top-k 순위는 크게 변할 수 있어 context drift가 커질 수 있음
    - 따라서 "lexical로는 크게 안 멀어졌는데 retrieval context는 더 크게 이동"은 모순이 아니라 서로 다른 관측 축임

- 질문에 대한 답
    - "기존 쿼리와 similarity가 많이 차이나는가?"
        - 현재 지표(토큰 Jaccard) 기준으로는 **크게 차이나지 않음**
    - "가져오는 context가 많이 달라지는가?"
        - exploit 대비 explore는 **더 많이 바꾸지만**, explore/exploit 후보 자체는 **중간 정도 겹침**
    - "프롬프팅만으로 explore/exploit 분리가 되나?"
        - action 선택 분리는 실제로 발생함(평균 explore 12.9%), 다만 분리 강도는 매우 크지 않고 context도 일부 공통됨





# Retro* 방법

1 RETRO*는 relevance estimation 쪽이다

주어진 candidate 문서 각각에 대해 relevance를 잘 점수화해서 rerank한다.
test time에 여러 reasoning trajectory를 만들고 score integration으로 안정화도 한다.

여기까지는 기본적으로 “있기만 하면 잘 고른다”에 강하다.

2 너는 evidence discovery 쪽이다

너의 2 hop 설명이 여기서 힘을 발휘해.
gold document가 답 자체가 아니라 답을 뒷받침하는 근거 문서라는 설정이면
query와 gold 사이가 직접 매칭이 아니라는 게 구조적인 문제로 된다.

그래서 너는 abstraction을 “점수화”에 쓰기보다
다음 두 가지에 쓰는 게 핵심이 된다.

evidence에 필요한 bridging concept을 생성해서 검색을 바꾼다
즉 evidence planning을 query rewriting으로 구현한다

corpus tree에서 branch hit을 coarse locator로 쓰고
그 아래 leaf로 내려가서 candidate set 자체를 바꾼다
즉 tree guided adaptive retrieval로 candidate generation을 확장한다

