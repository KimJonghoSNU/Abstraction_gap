<Overall flow>
- 나의 이전 연구 REPAIR와 비교
- REPAIR: reranking + adaptive retrieval for each listwise reranking window
    - 이 때 reranking할 때 think는 query에 대한 답이 뭘지 작성
    - corpus는 비슷한 내용끼리 연결한 그래프

- 큰 방향은 
    - 기존 연구에서 plan = query에 대한 답이 뭘지 작성하는 것
    - 지금은 abstraction 을 해야한다 = query에 대한 답이 있을 때 어떤 원리/근거에 기반해서 작성된 답인지 찾는 게 핵심이다
- 변경점
    - query plan -> query abstraction 으로 바뀌고
    - corpus도 비슷한 내용끼리 연결한 (수평적) graph로 표현하는 게 아니라 abstraction한 (계층적) tree 구조로 표현하는 방법
    - 이유: branch (=summarization) 이 있어야 domain-specific한 지식을 모델이 갖고 있지 않더라도 explore / exploit 판단 가능
- 원하는 contribution
    - Abstractive query rewriting (QR) & Corpus tree graph (CG) 사이의 interaction

- 실패한 방향: 
    - branch node -> query rewriting의 context로 활용
    - branch node들로 abstraction schema 만들려고 했는데 실패함, branch node정보가 query rewriting에 도움되지 않음

Tree정보를 어떻게 활용할까?
- 가정 1: domain-specific knowledge가 없을 때, abstractive 개념만 브랜치 레벨에서 매칭해준다면 해당 브랜치 밑에 gold doc 있을 것이다
    - If top5 is [leaf1, leaf2, branch3, branch4, leaf5], convert to [leaf1, leaf2, highest-score leaf under branch3, highest-score leaf under branch4, leaf5]
    - [표] 평균은 조금 오른다 (branch -> best leaf)


| Method                                   | bio   | earth | econ  | psych | robot | avg   |
|------------------------------------------|-------|-------|-------|-------|-------|-------|
| Our iterative rewriting EXPLOIT/EXPLORE  | 60.23 | 58.57 | 28.57 | 44.54 | 26.89 | 43.76 |
| branch -> best leaf                      | 63.41 | 59.1  | 26.56 | 43.65 | 27.88 | 44.12 |


다시 큰그림 보자 Tree를 뭐에 쓰냐?
- 1) Retrieval 결과를 보고, adaptive retrieval (44.12)
- 2) REPAIR에서: adaptive retrieval을 해당 문서 기반으로 할지 말지 결정. 마찬가지로, query rewriter가 기존 방향으로 계속 rewriting할지, 아니면 다른 방향으로 꺾어서 rewriting 할지를 도와줄 수 있을 것. 다른 방향에 뭐가 있는지에 대해 정보가 없으니까, 그 정보를 corpus tree로 제공해줄 수 있다는 것: 앗..이거 잘 못한다.




# devlog
  - src/run_round3_oracle.py
      - --round3_action_oracle now expects select/rerank/none (normalized via _normalize_oracle_mode).
      - For each action candidate we now store anchor top‑10 paths in oracle_action_anchor_top10.
      - LLM judge:
          - select: chooses explore vs exploit using new prompt, then uses that action’s top‑10 for both ndcg and next‑round context.
          - rerank: LLM reranks merged top‑10 (E1..E10 + X1..X10), then we use the reranked list for ndcg + next‑round context.
      - Added a one‑line comment for the important decision: in rerank, the next‑round rewrite action is set by majority of labels in top‑10 (ties → exploit).
  - src/rewrite_prompts.py
      - Added round3_action_oracle_select_v1 and round3_action_oracle_rerank_v1.
  - scripts/compute_query_signal_stats.py
      - Now computes overlap/drift@10 in the script using:
          - anchor_top_paths (current anchor) vs oracle_action_anchor_top10 (per‑action candidate anchor top‑10).
  - src/bash/round3/oracle_round3_action.sh
      - Updated to --round3_action_oracle select.


## Round3 action-oracle (select/rerank) 결과 요약
source: results/BRIGHT/ndcg_summaryround3_action_oracle.csv

- 모든 실험에서 iter0 nDCG는 동일(기본 anchor 쿼리 기준).
  - bio 41.96 / earth 45.44 / econ 21.25 / psych 34.37 / robot 21.61 (avg 32.93)
- max nDCG 기준 평균:
  - rerank: 43.61 (bio 61.11, earth 57.16, econ 29.71, psych 42.68, robot 27.40)
  - select: 43.33 (bio 58.96, earth 59.24, econ 29.37, psych 42.15, robot 26.91)
  - RAO=True (legacy boolean run): 50.71 (bio 68.82, earth 65.65, econ 36.09, psych 50.54, robot 32.46)
- 해석:
  - select/rerank는 **baseline oracle(ndcg)** 대비 성능이 낮음.
  - 두 LLM 방식 중 rerank가 약간 높지만 차이는 작음.
  - iter max 시점은 dataset별로 1~9 사이로 분산 (select는 비교적 빠른 iter에서 peak).

## Round3 action-oracle metric 정리 (2026-01-28)
- `Oracle_nDCG@10`: **explore vs exploit 중 gold 기준 best** (oracle upper bound).
- `nDCG@10`: **실제로 사용한 mode 결과** (select / rerank / ndcg / rrf).
- 제거: `OracleAction_nDCG@10`, `OracleExploit_nDCG@10`, `OracleExplore_nDCG@10`.
- 신규 mode: `rrf` (explore + exploit 결과를 RRF로 fuse, 그 결과를 nDCG와 next‑iter context로 사용).
